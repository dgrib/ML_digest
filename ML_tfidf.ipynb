{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d44db214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae4f1779",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>article_content</th>\n",
       "      <th>category</th>\n",
       "      <th>content_type</th>\n",
       "      <th>digest_number</th>\n",
       "      <th>download_status</th>\n",
       "      <th>dt</th>\n",
       "      <th>gather_dt</th>\n",
       "      <th>is_main</th>\n",
       "      <th>keywords</th>\n",
       "      <th>language</th>\n",
       "      <th>model</th>\n",
       "      <th>pk</th>\n",
       "      <th>processed</th>\n",
       "      <th>processed_tags</th>\n",
       "      <th>projects</th>\n",
       "      <th>state</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6139f2ccc7fae6a4e67e8db8</td>\n",
       "      <td>Today, Google launched its second cloud region...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>article_tag</td>\n",
       "      <td>NaN</td>\n",
       "      <td>success</td>\n",
       "      <td>2021-07-15T13:09:13Z</td>\n",
       "      <td>2021-07-16T06:17:58.965Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>gatherer.digestrecord</td>\n",
       "      <td>11475</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>FILTERED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Google Launches Second Cloud Region In India</td>\n",
       "      <td>https://analyticsindiamag.com/google-launches-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6139f2ccc7fae6a4e67e8db9</td>\n",
       "      <td>US-based Dun &amp; Bradstreet provides commercial ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>article_tag</td>\n",
       "      <td>NaN</td>\n",
       "      <td>success</td>\n",
       "      <td>2021-07-15T11:30:00Z</td>\n",
       "      <td>2021-07-16T06:17:58.975Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>gatherer.digestrecord</td>\n",
       "      <td>11476</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>FILTERED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Interview: Avinash Gupta, Managing Director &amp; ...</td>\n",
       "      <td>https://analyticsindiamag.com/interview-avinas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6139f2ccc7fae6a4e67e8dba</td>\n",
       "      <td>For AI-based software to work well in the real...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>article_tag</td>\n",
       "      <td>NaN</td>\n",
       "      <td>success</td>\n",
       "      <td>2021-07-15T08:30:00Z</td>\n",
       "      <td>2021-07-16T06:17:58.985Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>gatherer.digestrecord</td>\n",
       "      <td>11477</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>FILTERED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How Does Crowdsourcing Data Labelling Work?</td>\n",
       "      <td>https://analyticsindiamag.com/how-does-crowdso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  6139f2ccc7fae6a4e67e8db8   \n",
       "1  6139f2ccc7fae6a4e67e8db9   \n",
       "2  6139f2ccc7fae6a4e67e8dba   \n",
       "\n",
       "                                     article_content category content_type  \\\n",
       "0  Today, Google launched its second cloud region...      NaN  article_tag   \n",
       "1  US-based Dun & Bradstreet provides commercial ...      NaN  article_tag   \n",
       "2  For AI-based software to work well in the real...      NaN  article_tag   \n",
       "\n",
       "   digest_number download_status                    dt  \\\n",
       "0            NaN         success  2021-07-15T13:09:13Z   \n",
       "1            NaN         success  2021-07-15T11:30:00Z   \n",
       "2            NaN         success  2021-07-15T08:30:00Z   \n",
       "\n",
       "                  gather_dt is_main keywords language                  model  \\\n",
       "0  2021-07-16T06:17:58.965Z     NaN      NaN  ENGLISH  gatherer.digestrecord   \n",
       "1  2021-07-16T06:17:58.975Z     NaN      NaN  ENGLISH  gatherer.digestrecord   \n",
       "2  2021-07-16T06:17:58.985Z     NaN      NaN  ENGLISH  gatherer.digestrecord   \n",
       "\n",
       "      pk  processed processed_tags projects     state subcategory  \\\n",
       "0  11475      False             []      [1]  FILTERED         NaN   \n",
       "1  11476      False             []      [1]  FILTERED         NaN   \n",
       "2  11477      False             []      [1]  FILTERED         NaN   \n",
       "\n",
       "                                               title  \\\n",
       "0       Google Launches Second Cloud Region In India   \n",
       "1  Interview: Avinash Gupta, Managing Director & ...   \n",
       "2        How Does Crowdsourcing Data Labelling Work?   \n",
       "\n",
       "                                                 url  \n",
       "0  https://analyticsindiamag.com/google-launches-...  \n",
       "1  https://analyticsindiamag.com/interview-avinas...  \n",
       "2  https://analyticsindiamag.com/how-does-crowdso...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"simple_collection.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b4058e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_content</th>\n",
       "      <th>category</th>\n",
       "      <th>content_type</th>\n",
       "      <th>digest_number</th>\n",
       "      <th>download_status</th>\n",
       "      <th>is_main</th>\n",
       "      <th>keywords</th>\n",
       "      <th>language</th>\n",
       "      <th>model</th>\n",
       "      <th>pk</th>\n",
       "      <th>processed</th>\n",
       "      <th>processed_tags</th>\n",
       "      <th>projects</th>\n",
       "      <th>state</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Today, Google launched its second cloud region...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>article_tag</td>\n",
       "      <td>NaN</td>\n",
       "      <td>success</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>gatherer.digestrecord</td>\n",
       "      <td>11475</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>FILTERED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Google Launches Second Cloud Region In India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US-based Dun &amp; Bradstreet provides commercial ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>article_tag</td>\n",
       "      <td>NaN</td>\n",
       "      <td>success</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>gatherer.digestrecord</td>\n",
       "      <td>11476</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>FILTERED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Interview: Avinash Gupta, Managing Director &amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For AI-based software to work well in the real...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>article_tag</td>\n",
       "      <td>NaN</td>\n",
       "      <td>success</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>gatherer.digestrecord</td>\n",
       "      <td>11477</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>FILTERED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How Does Crowdsourcing Data Labelling Work?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     article_content category content_type  \\\n",
       "0  Today, Google launched its second cloud region...      NaN  article_tag   \n",
       "1  US-based Dun & Bradstreet provides commercial ...      NaN  article_tag   \n",
       "2  For AI-based software to work well in the real...      NaN  article_tag   \n",
       "\n",
       "   digest_number download_status is_main keywords language  \\\n",
       "0            NaN         success     NaN      NaN  ENGLISH   \n",
       "1            NaN         success     NaN      NaN  ENGLISH   \n",
       "2            NaN         success     NaN      NaN  ENGLISH   \n",
       "\n",
       "                   model     pk  processed processed_tags projects     state  \\\n",
       "0  gatherer.digestrecord  11475      False             []      [1]  FILTERED   \n",
       "1  gatherer.digestrecord  11476      False             []      [1]  FILTERED   \n",
       "2  gatherer.digestrecord  11477      False             []      [1]  FILTERED   \n",
       "\n",
       "  subcategory                                              title  \n",
       "0         NaN       Google Launches Second Cloud Region In India  \n",
       "1         NaN  Interview: Avinash Gupta, Managing Director & ...  \n",
       "2         NaN        How Does Crowdsourcing Data Labelling Work?  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['dt', 'gather_dt', '_id', 'url'])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8947ec1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "success    7537\n",
       "failed      123\n",
       "Name: download_status, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.download_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f61226e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7537, 16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# удалили незагруженные статьи\n",
    "\n",
    "df = df.drop(df.loc[df.download_status == 'failed'].index)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1cb4c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7312, 16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# удалили где нет статей\n",
    "\n",
    "df = df.drop(df.loc[df.article_content.isna()].index)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a873f191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ENGLISH    6901\n",
       "RUSSIAN     411\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ff32511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_russian = df.loc[df.language == 'RUSSIAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "627fdc5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5617bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_tag        5361\n",
       "not_article_tag    1951\n",
       "Name: content_type, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.content_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61cc6b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#предобработка текстов\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.tokenize import word_tokenize\n",
    "\n",
    "from razdel import tokenize # https://github.com/natasha/razdel\n",
    "#!pip install razdel\n",
    "\n",
    "import pymorphy2  # pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37f6582f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "stopword_ru = stopwords.words('russian')\n",
    "print(len(stopword_ru))\n",
    "stopword_eng = stopwords.words('english')\n",
    "print(len(stopword_eng))\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38cbcade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "990"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('stopwords.txt') as f:\n",
    "    additional_stopwords = [w.strip() for w in f.readlines() if w]\n",
    "stopwords = additional_stopwords + stopword_ru + stopword_eng\n",
    "\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "212cdb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    очистка текста\n",
    "    \n",
    "    на выходе очищеный текст\n",
    "    \n",
    "    '''\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.strip('\\n').strip('\\r').strip('\\t')\n",
    "    text = re.sub(\"-\\s\\r\\n\\|-\\s\\r\\n|\\r\\n\", '', str(text))\n",
    "\n",
    "    text = re.sub(\"[0-9]|[-—.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|�\", '', text)\n",
    "    text = re.sub(r\"\\r\\n\\t|\\n|\\\\s|\\r\\t|\\\\n\", ' ', text)\n",
    "    text = re.sub(r'[\\xad]|[\\s+]', ' ', text.strip())\n",
    "    \n",
    "    #tokens = list(tokenize(text))\n",
    "    #words = [_.text for _ in tokens]\n",
    "    #words = [w for w in words if w not in stopword_ru]\n",
    "    \n",
    "    #return \" \".join(words)\n",
    "    return text\n",
    "\n",
    "cache = {}\n",
    "\n",
    "def lemmatization(text):\n",
    "    '''\n",
    "    лемматизация\n",
    "        [0] если зашел тип не `str` делаем его `str`\n",
    "        [1] токенизация предложения через razdel\n",
    "        [2] проверка есть ли в начале слова '-'\n",
    "        [3] проверка токена с одного символа\n",
    "        [4] проверка есть ли данное слово в кэше\n",
    "        [5] лемматизация слова\n",
    "        [6] проверка на стоп-слова\n",
    "\n",
    "    на выходе лист отлемматизированых токенов\n",
    "    '''\n",
    "\n",
    "    # [0]\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # [1]\n",
    "    tokens = list(tokenize(text))\n",
    "    words = [_.text for _ in tokens]\n",
    "\n",
    "    words_lem = []\n",
    "    for w in words:\n",
    "        if w[0] == '-': # [2]\n",
    "            w = w[1:]\n",
    "        if len(w)>1: # [3]\n",
    "            if w in cache: # [4]\n",
    "                words_lem.append(cache[w])\n",
    "            else: # [5]\n",
    "                temp_cach = cache[w] = morph.parse(w)[0].normal_form \n",
    "                # 0 скоре самый максимальный - остальные нормальные формы первая вторая и тд  реже встречаются\n",
    "                words_lem.append(temp_cach)\n",
    "    \n",
    "    words_lem_without_stopwords=[i for i in words_lem if not i in stopwords] # [6]\n",
    "    \n",
    "    return words_lem_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5921ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_russian = df.loc[df.language == 'RUSSIAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32f9b720",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 133 ms, sys: 3.06 ms, total: 136 ms\n",
      "Wall time: 135 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-a831bfacbccc>:15: FutureWarning: Possible nested set at position 39\n",
      "  text = re.sub(\"[0-9]|[-—.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|�\", '', text)\n",
      "<timed exec>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Запускаем очистку текста. Будет долго...\n",
    "df_russian['article_content'] = df_russian['article_content'].apply(lambda x: clean_text(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e6cbf8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #Запускаем очистку текста. Будет долго...\n",
    "# df['article_content'] = df['article_content'].apply(lambda x: clean_text(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75a82f9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #Запускаем лемматизацию текста. Будет очень долго...\n",
    "# df['article_content'] = df['article_content'].apply(lambda x: lemmatization(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79a42621",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.5 s, sys: 34.6 ms, total: 7.54 s\n",
      "Wall time: 7.54 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Запускаем лемматизацию текста. Будет очень долго...\n",
    "df_russian['article_content'] = df_russian['article_content'].apply(lambda x: lemmatization(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd2fa18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#сформируем список наших текстов, разбив еще и на пробелы\n",
    "texts = [t for t in df_russian['article_content'].values]\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in texts] \n",
    "# common_corpus это зеленая матрица - для каждого документа он будет строить что за слова в каждом документе\n",
    "# текст каждого документа бросаем в doc2bow в наш common_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1894d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ece7a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_number = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "975f2913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.6 s, sys: 7.37 s, total: 38 s\n",
      "Wall time: 5.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import LdaModel\n",
    "# Train the model on the corpus.\n",
    "\n",
    "# это не только предопределение нашей модели - это сразу и обучение!!!\n",
    "lda = LdaModel(common_corpus, num_topics=topics_number, id2word=common_dictionary, passes=10) \n",
    "# passes - сколько раз будем сдвигать центроиды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bab9ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "# Save model to disk.\n",
    "temp_file = datapath(\"model.lda\")\n",
    "lda.save(temp_file)\n",
    "\n",
    "# Load a potentially pretrained model from disk.\n",
    "lda = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58ad1329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['государственный', 'департамент', 'сша', 'предлагать', 'вознаграждение', 'размер', 'миллион', 'информация', 'позволить', 'установить', 'личность', 'киберпреступник', 'атаковать', 'критически', 'важный', 'инфраструктура', 'программвымогатель', 'усилие', 'борьба', 'угроза', 'криптовымогатель', 'координировать', 'целевой', 'белый', 'домомправительство', 'запустить', 'stopransomwaregov', 'предлагать', 'помощь', 'противодействие', 'атака', 'повышение', 'безопасность', 'сеть', 'шаг', 'запуск', 'сеть', 'борьба', 'финансовый', 'преступление', 'министерство', 'финансы', 'структура', 'банк', 'технологический', 'сторона', 'цель', 'эффективный', 'мера', 'борьба', 'отмывание', 'деньга', 'криптовалюта', 'пытаться', 'отслеживать', 'путь', 'средство', 'выплатить', 'качество', 'выкуп', 'злоумышленник', 'фбр', 'возмещение', 'часть', 'выкуп', 'миллион', 'уплатить', 'colonial', 'pipeline', 'маевознаграждение', 'выплачиваться', 'рамка', 'программа', 'государственный', 'департамент', 'вознаграждение', 'правосудие', 'заявление', 'правительство', 'защита', 'информатор', 'предложный', 'механизм', 'передача', 'сведение', 'даркнуть', 'вознаграждение', 'включать', 'криптовалютупредставитель', 'администрация', 'комментировать', 'причастный', 'правительство', 'сша', 'исчезновение', 'даркнета', 'группировка', 'revil', 'недоступный', 'примерно', 'утро', 'московский', 'посещение', 'страница', 'размещаться', 'новость', 'заявление', 'группировка', 'увидеть', 'объявление', 'страница', 'найти']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(24, 0.9915011)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new corpus, made of previously unseen documents.\n",
    "other_texts = [t for t in df_russian['article_content'].iloc[:3]]\n",
    "other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]\n",
    "\n",
    "unseen_doc = other_corpus[2]\n",
    "print(other_texts[2])\n",
    "lda[unseen_doc] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e48e065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_0: dect станция трубка базовый containerd контейнер менеджер\n",
      "topic_1: сборка тред скафандр новость телескоп канал космический\n",
      "topic_2: конференция язык спо julia стандарт свободный спутник\n",
      "topic_3: требование безопасность ansible язык оценка оуд разработка\n",
      "topic_4: млрд apple право россия рынок драйвер китай\n",
      "topic_5: api linux версия kubernetes сообщество python скрипт\n",
      "topic_6: dmca вуз звезда студент университет нейтронный amazon\n",
      "topic_7: резервный квантовый функция поток ibm копия таймер\n",
      "topic_8: linux astra база мониторинг очень zimbramtablockedextension письмо\n",
      "topic_9: facebook apple оператор мир оборудование клиент trueconf\n",
      "topic_10: vpn ipsec настройка доступ безопасность адрес подключение\n",
      "topic_11: объект материал узел параметр рендёр окно камера\n",
      "topic_12: git репозиторий annex версия поддержка deckhouse кластер\n",
      "topic_13: узел kubernetes сеть кластер сборка gitlab тема\n",
      "topic_14: млрд руб млн google безопасность документ информация\n",
      "topic_15: программа вредоносный раздел linux устройство версия autoruns\n",
      "topic_16: кварк частица github сила объект помощь помочь\n",
      "topic_17: практика модель метрика функция зрелость баг prometheus\n",
      "topic_18: linux драйвер ядро helm хороший вообще поддержка\n",
      "topic_19: gitlab ultimate premium devops free стадия оригинальный\n",
      "topic_20: twitter hcie fleets конкурс дакриолит indie intsights\n",
      "topic_21: функция сеть технология информация разработка facebook доступ\n",
      "topic_22: выручка вырасти игра руководитель финансовый навык продукт\n",
      "topic_23: смарткарта помощь документ ваш вход документация markdown\n",
      "topic_24: атака информация microsoft pegasus обнаружить список безопасность\n"
     ]
    }
   ],
   "source": [
    "x=lda.show_topics(num_topics=topics_number, num_words=7,formatted=False)\n",
    "topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "\n",
    "#Below Code Prints Only Words \n",
    "for topic,words in topics_words:\n",
    "    print(\"topic_{}: \".format(topic)+\" \".join(words))\n",
    "# получается это вывели топ 7 слов в каждой тематике по величине вероятности принадлежности к этой теме\n",
    "# такие вещи как nn надо заносить в стоп слова - снова прогоните - \n",
    "# и опять могут мусорные слова быть - это ноормально - это правильно для тематического моделирования\n",
    "# для тематич моделирования крайне важно сидеть и исключать мусорные токены - млн и тд\n",
    "# процесс постройки векторов долгий и итеративный\n",
    "\n",
    "# мы кидаем центроиды и если они будут скатываться к разным минимумам это тоже нормально - разные слова могут быть\n",
    "# изза стохастичности (кидаем центроиды) то постоянность не будет и скатываться будет в разне места\n",
    "\n",
    "# можно записать в отдельный список центроиды которые получили и \n",
    "# есть методы которые обучаются с нуля но имеют начальное расположение кластеров именно в этих центроидах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd2554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f418548c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
