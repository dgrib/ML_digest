{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8207bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary \n",
    "\n",
    "# from sklearn.pipeline import Pipeline, make_pipeline\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# import itertoy_test, predsols\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d969efb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>article_content</th>\n",
       "      <th>category</th>\n",
       "      <th>content_type</th>\n",
       "      <th>digest_number</th>\n",
       "      <th>download_status</th>\n",
       "      <th>dt</th>\n",
       "      <th>gather_dt</th>\n",
       "      <th>is_main</th>\n",
       "      <th>keywords</th>\n",
       "      <th>language</th>\n",
       "      <th>model</th>\n",
       "      <th>pk</th>\n",
       "      <th>processed</th>\n",
       "      <th>processed_tags</th>\n",
       "      <th>projects</th>\n",
       "      <th>state</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6139f2ccc7fae6a4e67e8db8</td>\n",
       "      <td>Today, Google launched its second cloud region...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>article_tag</td>\n",
       "      <td>NaN</td>\n",
       "      <td>success</td>\n",
       "      <td>2021-07-15T13:09:13Z</td>\n",
       "      <td>2021-07-16T06:17:58.965Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>gatherer.digestrecord</td>\n",
       "      <td>11475</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>FILTERED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Google Launches Second Cloud Region In India</td>\n",
       "      <td>https://analyticsindiamag.com/google-launches-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6139f2ccc7fae6a4e67e8db9</td>\n",
       "      <td>US-based Dun &amp; Bradstreet provides commercial ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>article_tag</td>\n",
       "      <td>NaN</td>\n",
       "      <td>success</td>\n",
       "      <td>2021-07-15T11:30:00Z</td>\n",
       "      <td>2021-07-16T06:17:58.975Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>gatherer.digestrecord</td>\n",
       "      <td>11476</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>FILTERED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Interview: Avinash Gupta, Managing Director &amp; ...</td>\n",
       "      <td>https://analyticsindiamag.com/interview-avinas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6139f2ccc7fae6a4e67e8dba</td>\n",
       "      <td>For AI-based software to work well in the real...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>article_tag</td>\n",
       "      <td>NaN</td>\n",
       "      <td>success</td>\n",
       "      <td>2021-07-15T08:30:00Z</td>\n",
       "      <td>2021-07-16T06:17:58.985Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>gatherer.digestrecord</td>\n",
       "      <td>11477</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>FILTERED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How Does Crowdsourcing Data Labelling Work?</td>\n",
       "      <td>https://analyticsindiamag.com/how-does-crowdso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  6139f2ccc7fae6a4e67e8db8   \n",
       "1  6139f2ccc7fae6a4e67e8db9   \n",
       "2  6139f2ccc7fae6a4e67e8dba   \n",
       "\n",
       "                                     article_content category content_type  \\\n",
       "0  Today, Google launched its second cloud region...      NaN  article_tag   \n",
       "1  US-based Dun & Bradstreet provides commercial ...      NaN  article_tag   \n",
       "2  For AI-based software to work well in the real...      NaN  article_tag   \n",
       "\n",
       "   digest_number download_status                    dt  \\\n",
       "0            NaN         success  2021-07-15T13:09:13Z   \n",
       "1            NaN         success  2021-07-15T11:30:00Z   \n",
       "2            NaN         success  2021-07-15T08:30:00Z   \n",
       "\n",
       "                  gather_dt is_main keywords language                  model  \\\n",
       "0  2021-07-16T06:17:58.965Z     NaN      NaN  ENGLISH  gatherer.digestrecord   \n",
       "1  2021-07-16T06:17:58.975Z     NaN      NaN  ENGLISH  gatherer.digestrecord   \n",
       "2  2021-07-16T06:17:58.985Z     NaN      NaN  ENGLISH  gatherer.digestrecord   \n",
       "\n",
       "      pk  processed processed_tags projects     state subcategory  \\\n",
       "0  11475      False             []      [1]  FILTERED         NaN   \n",
       "1  11476      False             []      [1]  FILTERED         NaN   \n",
       "2  11477      False             []      [1]  FILTERED         NaN   \n",
       "\n",
       "                                               title  \\\n",
       "0       Google Launches Second Cloud Region In India   \n",
       "1  Interview: Avinash Gupta, Managing Director & ...   \n",
       "2        How Does Crowdsourcing Data Labelling Work?   \n",
       "\n",
       "                                                 url  \n",
       "0  https://analyticsindiamag.com/google-launches-...  \n",
       "1  https://analyticsindiamag.com/interview-avinas...  \n",
       "2  https://analyticsindiamag.com/how-does-crowdso...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"simple_collection.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc44d65d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_content</th>\n",
       "      <th>category</th>\n",
       "      <th>content_type</th>\n",
       "      <th>digest_number</th>\n",
       "      <th>download_status</th>\n",
       "      <th>is_main</th>\n",
       "      <th>keywords</th>\n",
       "      <th>language</th>\n",
       "      <th>model</th>\n",
       "      <th>pk</th>\n",
       "      <th>processed</th>\n",
       "      <th>processed_tags</th>\n",
       "      <th>projects</th>\n",
       "      <th>state</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Today, Google launched its second cloud region...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>article_tag</td>\n",
       "      <td>NaN</td>\n",
       "      <td>success</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>gatherer.digestrecord</td>\n",
       "      <td>11475</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>FILTERED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Google Launches Second Cloud Region In India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US-based Dun &amp; Bradstreet provides commercial ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>article_tag</td>\n",
       "      <td>NaN</td>\n",
       "      <td>success</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>gatherer.digestrecord</td>\n",
       "      <td>11476</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>FILTERED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Interview: Avinash Gupta, Managing Director &amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For AI-based software to work well in the real...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>article_tag</td>\n",
       "      <td>NaN</td>\n",
       "      <td>success</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>gatherer.digestrecord</td>\n",
       "      <td>11477</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>FILTERED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How Does Crowdsourcing Data Labelling Work?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     article_content category content_type  \\\n",
       "0  Today, Google launched its second cloud region...      NaN  article_tag   \n",
       "1  US-based Dun & Bradstreet provides commercial ...      NaN  article_tag   \n",
       "2  For AI-based software to work well in the real...      NaN  article_tag   \n",
       "\n",
       "   digest_number download_status is_main keywords language  \\\n",
       "0            NaN         success     NaN      NaN  ENGLISH   \n",
       "1            NaN         success     NaN      NaN  ENGLISH   \n",
       "2            NaN         success     NaN      NaN  ENGLISH   \n",
       "\n",
       "                   model     pk  processed processed_tags projects     state  \\\n",
       "0  gatherer.digestrecord  11475      False             []      [1]  FILTERED   \n",
       "1  gatherer.digestrecord  11476      False             []      [1]  FILTERED   \n",
       "2  gatherer.digestrecord  11477      False             []      [1]  FILTERED   \n",
       "\n",
       "  subcategory                                              title  \n",
       "0         NaN       Google Launches Second Cloud Region In India  \n",
       "1         NaN  Interview: Avinash Gupta, Managing Director & ...  \n",
       "2         NaN        How Does Crowdsourcing Data Labelling Work?  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['dt', 'gather_dt', '_id', 'url'])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6d33f88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "success    7537\n",
       "failed      123\n",
       "Name: download_status, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.download_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75598eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7537, 16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# удалили незагруженные статьи\n",
    "\n",
    "df = df.drop(df.loc[df.download_status == 'failed'].index)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66645f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7312, 16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# удалили где нет статей\n",
    "\n",
    "df = df.drop(df.loc[df.article_content.isna()].index)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50cd0e7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ENGLISH    6901\n",
       "RUSSIAN     411\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0dfa633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_russian = df.loc[df.language == 'RUSSIAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4929f26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5a8b22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_tag        5361\n",
       "not_article_tag    1951\n",
       "Name: content_type, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.content_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fc46ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#предобработка текстов\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.tokenize import word_tokenize\n",
    "\n",
    "from razdel import tokenize # https://github.com/natasha/razdel\n",
    "#!pip install razdel\n",
    "\n",
    "import pymorphy2  # pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a4abde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n"
     ]
    }
   ],
   "source": [
    "stopword_ru = stopwords.words('russian')\n",
    "print(len(stopword_ru))\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b296e076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "stopword_eng = stopwords.words('english')\n",
    "print(len(stopword_eng))\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "226d334e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "990"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('stopwords.txt') as f:\n",
    "    additional_stopwords = [w.strip() for w in f.readlines() if w]\n",
    "stopwords = additional_stopwords + stopword_ru + stopword_eng\n",
    "\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09afa5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    очистка текста\n",
    "    \n",
    "    на выходе очищеный текст\n",
    "    \n",
    "    '''\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.strip('\\n').strip('\\r').strip('\\t')\n",
    "    text = re.sub(\"-\\s\\r\\n\\|-\\s\\r\\n|\\r\\n\", '', str(text))\n",
    "\n",
    "    text = re.sub(\"[0-9]|[-—.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|�\", '', text)\n",
    "    text = re.sub(r\"\\r\\n\\t|\\n|\\\\s|\\r\\t|\\\\n\", ' ', text)\n",
    "    text = re.sub(r'[\\xad]|[\\s+]', ' ', text.strip())\n",
    "    \n",
    "    #tokens = list(tokenize(text))\n",
    "    #words = [_.text for _ in tokens]\n",
    "    #words = [w for w in words if w not in stopword_ru]\n",
    "    \n",
    "    #return \" \".join(words)\n",
    "    return text\n",
    "\n",
    "cache = {}\n",
    "\n",
    "def lemmatization(text):\n",
    "    '''\n",
    "    лемматизация\n",
    "        [0] если зашел тип не `str` делаем его `str`\n",
    "        [1] токенизация предложения через razdel\n",
    "        [2] проверка есть ли в начале слова '-'\n",
    "        [3] проверка токена с одного символа\n",
    "        [4] проверка есть ли данное слово в кэше\n",
    "        [5] лемматизация слова\n",
    "        [6] проверка на стоп-слова\n",
    "\n",
    "    на выходе лист отлемматизированых токенов\n",
    "    '''\n",
    "\n",
    "    # [0]\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # [1]\n",
    "    tokens = list(tokenize(text))\n",
    "    words = [_.text for _ in tokens]\n",
    "\n",
    "    words_lem = []\n",
    "    for w in words:\n",
    "        if w[0] == '-': # [2]\n",
    "            w = w[1:]\n",
    "        if len(w)>1: # [3]\n",
    "            if w in cache: # [4]\n",
    "                words_lem.append(cache[w])\n",
    "            else: # [5]\n",
    "                temp_cach = cache[w] = morph.parse(w)[0].normal_form \n",
    "                # 0 скоре самый максимальный - остальные нормальные формы первая вторая и тд  реже встречаются\n",
    "                words_lem.append(temp_cach)\n",
    "    \n",
    "    words_lem_without_stopwords=[i for i in words_lem if not i in stopwords] # [6]\n",
    "    \n",
    "    return words_lem_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57b92384",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_russian = df.loc[df.language == 'ENGLISH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab0b2b1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-a831bfacbccc>:15: FutureWarning: Possible nested set at position 39\n",
      "  text = re.sub(\"[0-9]|[-—.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|�\", '', text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.95 s, sys: 64.7 ms, total: 3.01 s\n",
      "Wall time: 3.01 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Запускаем очистку текста. Будет долго...\n",
    "df_russian['article_content'] = df_russian['article_content'].apply(lambda x: clean_text(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf31c99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #Запускаем очистку текста. Будет долго...\n",
    "# df['article_content'] = df['article_content'].apply(lambda x: clean_text(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9defeac6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #Запускаем лемматизацию текста. Будет очень долго...\n",
    "# df['article_content'] = df['article_content'].apply(lambda x: lemmatization(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf9148ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "no such name",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/shapes.py\u001b[0m in \u001b[0;36mis_latin_char\u001b[0;34m(uchr)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_latin_letters_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muchr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '𘂇'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4136\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4137\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4138\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-a831bfacbccc>\u001b[0m in \u001b[0;36mlemmatization\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mwords_lem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# [5]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mtemp_cach\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmorph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0;31m# 0 скоре самый максимальный - остальные нормальные формы первая вторая и тд  реже встречаются\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mwords_lem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_cach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/analyzer.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_terminal\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_units\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_terminal\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/units/by_shape.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, word, word_lower, seen_parses)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseen_parses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_lower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/units/by_shape.py\u001b[0m in \u001b[0;36mcheck_shape\u001b[0;34m(self, word, word_lower)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_lower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mis_latin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/shapes.py\u001b[0m in \u001b[0;36mis_latin\u001b[0;34m(token)\u001b[0m\n\u001b[1;32m     38\u001b[0m     return (\n\u001b[1;32m     39\u001b[0m         \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_latin_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/shapes.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     38\u001b[0m     return (\n\u001b[1;32m     39\u001b[0m         \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_latin_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/shapes.py\u001b[0m in \u001b[0;36mis_latin_char\u001b[0;34m(uchr)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muchr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0muchr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muchr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mis_latin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'LATIN'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muchr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_latin_letters_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muchr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_latin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: no such name"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Запускаем лемматизацию текста. Будет очень долго...\n",
    "df_russian['article_content'] = df_russian['article_content'].apply(lambda x: lemmatization(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22c51d66",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-2edf55d01784>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create a corpus from a list of texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcommon_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mcommon_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcommon_dictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# common_corpus это зеленая матрица - для каждого документа он будет строить что за слова в каждом документе\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             self.add_lifecycle_event(\n\u001b[1;32m     80\u001b[0m                 \u001b[0;34m\"created\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;31m# update Dictionary with the document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ignore the result, here we only care about updating token ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"built %s from %i documents (total %i corpus positions)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \"\"\"\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"doc2bow expects an array of unicode tokens on input, not a single string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;31m# Construct (word, frequency) mapping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "#сформируем список наших текстов, разбив еще и на пробелы\n",
    "texts = [t for t in df_russian['article_content'].values]\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in texts] \n",
    "# common_corpus это зеленая матрица - для каждого документа он будет строить что за слова в каждом документе\n",
    "# текст каждого документа бросаем в doc2bow в наш common_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33af593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660dbb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_number = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f0d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.models import LdaModel\n",
    "# Train the model on the corpus.\n",
    "\n",
    "# это не только предопределение нашей модели - это сразу и обучение!!!\n",
    "lda = LdaModel(common_corpus, num_topics=topics_number, id2word=common_dictionary)#, passes=10) \n",
    "# passes - сколько раз будем сдвигать центроиды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaed4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "# Save model to disk.\n",
    "temp_file = datapath(\"model.lda\")\n",
    "lda.save(temp_file)\n",
    "\n",
    "# Load a potentially pretrained model from disk.\n",
    "lda = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e7260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new corpus, made of previously unseen documents.\n",
    "other_texts = [t for t in df_russian['article_content'].iloc[:3]]\n",
    "other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]\n",
    "\n",
    "unseen_doc = other_corpus[2]\n",
    "print(other_texts[2])\n",
    "lda[unseen_doc] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2170e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=lda.show_topics(num_topics=topics_number, num_words=7,formatted=False)\n",
    "topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "\n",
    "#Below Code Prints Only Words \n",
    "for topic,words in topics_words:\n",
    "    print(\"topic_{}: \".format(topic)+\" \".join(words))\n",
    "# получается это вывели топ 7 слов в каждой тематике по величине вероятности принадлежности к этой теме\n",
    "# такие вещи как nn надо заносить в стоп слова - снова прогоните - \n",
    "# и опять могут мусорные слова быть - это ноормально - это правильно для тематического моделирования\n",
    "# для тематич моделирования крайне важно сидеть и исключать мусорные токены - млн и тд\n",
    "# процесс постройки векторов долгий и итеративный\n",
    "\n",
    "# мы кидаем центроиды и если они будут скатываться к разным минимумам это тоже нормально - разные слова могут быть\n",
    "# изза стохастичности (кидаем центроиды) то постоянность не будет и скатываться будет в разне места\n",
    "\n",
    "# можно записать в отдельный список центроиды которые получили и \n",
    "# есть методы которые обучаются с нуля но имеют начальное расположение кластеров именно в этих центроидах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539239a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a287aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
