{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8207bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary \n",
    "\n",
    "# from sklearn.pipeline import Pipeline, make_pipeline\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# import itertoy_test, predsols\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d969efb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>article_content</th>\n",
       "      <th>category</th>\n",
       "      <th>content_type</th>\n",
       "      <th>digest_number</th>\n",
       "      <th>download_status</th>\n",
       "      <th>dt</th>\n",
       "      <th>gather_dt</th>\n",
       "      <th>is_main</th>\n",
       "      <th>keywords</th>\n",
       "      <th>language</th>\n",
       "      <th>model</th>\n",
       "      <th>pk</th>\n",
       "      <th>processed</th>\n",
       "      <th>processed_tags</th>\n",
       "      <th>projects</th>\n",
       "      <th>state</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6139f2ccc7fae6a4e67e8db8</td>\n",
       "      <td>Today, Google launched its second cloud region...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>article_tag</td>\n",
       "      <td>NaN</td>\n",
       "      <td>success</td>\n",
       "      <td>2021-07-15T13:09:13Z</td>\n",
       "      <td>2021-07-16T06:17:58.965Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>gatherer.digestrecord</td>\n",
       "      <td>11475</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>FILTERED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Google Launches Second Cloud Region In India</td>\n",
       "      <td>https://analyticsindiamag.com/google-launches-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6139f2ccc7fae6a4e67e8db9</td>\n",
       "      <td>US-based Dun &amp; Bradstreet provides commercial ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>article_tag</td>\n",
       "      <td>NaN</td>\n",
       "      <td>success</td>\n",
       "      <td>2021-07-15T11:30:00Z</td>\n",
       "      <td>2021-07-16T06:17:58.975Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>gatherer.digestrecord</td>\n",
       "      <td>11476</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>FILTERED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Interview: Avinash Gupta, Managing Director &amp; ...</td>\n",
       "      <td>https://analyticsindiamag.com/interview-avinas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6139f2ccc7fae6a4e67e8dba</td>\n",
       "      <td>For AI-based software to work well in the real...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>article_tag</td>\n",
       "      <td>NaN</td>\n",
       "      <td>success</td>\n",
       "      <td>2021-07-15T08:30:00Z</td>\n",
       "      <td>2021-07-16T06:17:58.985Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>gatherer.digestrecord</td>\n",
       "      <td>11477</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>FILTERED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How Does Crowdsourcing Data Labelling Work?</td>\n",
       "      <td>https://analyticsindiamag.com/how-does-crowdso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  6139f2ccc7fae6a4e67e8db8   \n",
       "1  6139f2ccc7fae6a4e67e8db9   \n",
       "2  6139f2ccc7fae6a4e67e8dba   \n",
       "\n",
       "                                     article_content category content_type  \\\n",
       "0  Today, Google launched its second cloud region...      NaN  article_tag   \n",
       "1  US-based Dun & Bradstreet provides commercial ...      NaN  article_tag   \n",
       "2  For AI-based software to work well in the real...      NaN  article_tag   \n",
       "\n",
       "   digest_number download_status                    dt  \\\n",
       "0            NaN         success  2021-07-15T13:09:13Z   \n",
       "1            NaN         success  2021-07-15T11:30:00Z   \n",
       "2            NaN         success  2021-07-15T08:30:00Z   \n",
       "\n",
       "                  gather_dt is_main keywords language                  model  \\\n",
       "0  2021-07-16T06:17:58.965Z     NaN      NaN  ENGLISH  gatherer.digestrecord   \n",
       "1  2021-07-16T06:17:58.975Z     NaN      NaN  ENGLISH  gatherer.digestrecord   \n",
       "2  2021-07-16T06:17:58.985Z     NaN      NaN  ENGLISH  gatherer.digestrecord   \n",
       "\n",
       "      pk  processed processed_tags projects     state subcategory  \\\n",
       "0  11475      False             []      [1]  FILTERED         NaN   \n",
       "1  11476      False             []      [1]  FILTERED         NaN   \n",
       "2  11477      False             []      [1]  FILTERED         NaN   \n",
       "\n",
       "                                               title  \\\n",
       "0       Google Launches Second Cloud Region In India   \n",
       "1  Interview: Avinash Gupta, Managing Director & ...   \n",
       "2        How Does Crowdsourcing Data Labelling Work?   \n",
       "\n",
       "                                                 url  \n",
       "0  https://analyticsindiamag.com/google-launches-...  \n",
       "1  https://analyticsindiamag.com/interview-avinas...  \n",
       "2  https://analyticsindiamag.com/how-does-crowdso...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"simple_collection.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc44d65d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_content</th>\n",
       "      <th>category</th>\n",
       "      <th>content_type</th>\n",
       "      <th>digest_number</th>\n",
       "      <th>download_status</th>\n",
       "      <th>is_main</th>\n",
       "      <th>keywords</th>\n",
       "      <th>language</th>\n",
       "      <th>model</th>\n",
       "      <th>pk</th>\n",
       "      <th>processed</th>\n",
       "      <th>processed_tags</th>\n",
       "      <th>projects</th>\n",
       "      <th>state</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Today, Google launched its second cloud region...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>article_tag</td>\n",
       "      <td>NaN</td>\n",
       "      <td>success</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>gatherer.digestrecord</td>\n",
       "      <td>11475</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>FILTERED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Google Launches Second Cloud Region In India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US-based Dun &amp; Bradstreet provides commercial ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>article_tag</td>\n",
       "      <td>NaN</td>\n",
       "      <td>success</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>gatherer.digestrecord</td>\n",
       "      <td>11476</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>FILTERED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Interview: Avinash Gupta, Managing Director &amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For AI-based software to work well in the real...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>article_tag</td>\n",
       "      <td>NaN</td>\n",
       "      <td>success</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>gatherer.digestrecord</td>\n",
       "      <td>11477</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>FILTERED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How Does Crowdsourcing Data Labelling Work?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     article_content category content_type  \\\n",
       "0  Today, Google launched its second cloud region...      NaN  article_tag   \n",
       "1  US-based Dun & Bradstreet provides commercial ...      NaN  article_tag   \n",
       "2  For AI-based software to work well in the real...      NaN  article_tag   \n",
       "\n",
       "   digest_number download_status is_main keywords language  \\\n",
       "0            NaN         success     NaN      NaN  ENGLISH   \n",
       "1            NaN         success     NaN      NaN  ENGLISH   \n",
       "2            NaN         success     NaN      NaN  ENGLISH   \n",
       "\n",
       "                   model     pk  processed processed_tags projects     state  \\\n",
       "0  gatherer.digestrecord  11475      False             []      [1]  FILTERED   \n",
       "1  gatherer.digestrecord  11476      False             []      [1]  FILTERED   \n",
       "2  gatherer.digestrecord  11477      False             []      [1]  FILTERED   \n",
       "\n",
       "  subcategory                                              title  \n",
       "0         NaN       Google Launches Second Cloud Region In India  \n",
       "1         NaN  Interview: Avinash Gupta, Managing Director & ...  \n",
       "2         NaN        How Does Crowdsourcing Data Labelling Work?  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['dt', 'gather_dt', '_id', 'url'])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6d33f88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "success    7537\n",
       "failed      123\n",
       "Name: download_status, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.download_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75598eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7537, 16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# —É–¥–∞–ª–∏–ª–∏ –Ω–µ–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏\n",
    "\n",
    "df = df.drop(df.loc[df.download_status == 'failed'].index)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66645f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7312, 16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# —É–¥–∞–ª–∏–ª–∏ –≥–¥–µ –Ω–µ—Ç —Å—Ç–∞—Ç–µ–π\n",
    "\n",
    "df = df.drop(df.loc[df.article_content.isna()].index)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50cd0e7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ENGLISH    6901\n",
       "RUSSIAN     411\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0dfa633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_russian = df.loc[df.language == 'RUSSIAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4929f26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5a8b22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_tag        5361\n",
       "not_article_tag    1951\n",
       "Name: content_type, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.content_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fc46ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#–ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.tokenize import word_tokenize\n",
    "\n",
    "from razdel import tokenize # https://github.com/natasha/razdel\n",
    "#!pip install razdel\n",
    "\n",
    "import pymorphy2  # pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a4abde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n"
     ]
    }
   ],
   "source": [
    "stopword_ru = stopwords.words('russian')\n",
    "print(len(stopword_ru))\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b296e076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "stopword_eng = stopwords.words('english')\n",
    "print(len(stopword_eng))\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "226d334e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "990"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('stopwords.txt') as f:\n",
    "    additional_stopwords = [w.strip() for w in f.readlines() if w]\n",
    "stopwords = additional_stopwords + stopword_ru + stopword_eng\n",
    "\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09afa5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\n",
    "    \n",
    "    –Ω–∞ –≤—ã—Ö–æ–¥–µ –æ—á–∏—â–µ–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
    "    \n",
    "    '''\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.strip('\\n').strip('\\r').strip('\\t')\n",
    "    text = re.sub(\"-\\s\\r\\n\\|-\\s\\r\\n|\\r\\n\", '', str(text))\n",
    "\n",
    "    text = re.sub(\"[0-9]|[-‚Äî.,:;_%¬©¬´¬ª?*!@#‚Ññ$^‚Ä¢¬∑&()]|[+=]|[[]|[]]|[/]|ÔøΩ\", '', text)\n",
    "    text = re.sub(r\"\\r\\n\\t|\\n|\\\\s|\\r\\t|\\\\n\", ' ', text)\n",
    "    text = re.sub(r'[\\xad]|[\\s+]', ' ', text.strip())\n",
    "    \n",
    "    #tokens = list(tokenize(text))\n",
    "    #words = [_.text for _ in tokens]\n",
    "    #words = [w for w in words if w not in stopword_ru]\n",
    "    \n",
    "    #return \" \".join(words)\n",
    "    return text\n",
    "\n",
    "cache = {}\n",
    "\n",
    "def lemmatization(text):\n",
    "    '''\n",
    "    –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\n",
    "        [0] –µ—Å–ª–∏ –∑–∞—à–µ–ª —Ç–∏–ø –Ω–µ `str` –¥–µ–ª–∞–µ–º –µ–≥–æ `str`\n",
    "        [1] —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ razdel\n",
    "        [2] –ø—Ä–æ–≤–µ—Ä–∫–∞ –µ—Å—Ç—å –ª–∏ –≤ –Ω–∞—á–∞–ª–µ —Å–ª–æ–≤–∞ '-'\n",
    "        [3] –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫–µ–Ω–∞ —Å –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞\n",
    "        [4] –ø—Ä–æ–≤–µ—Ä–∫–∞ –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω–æ–µ —Å–ª–æ–≤–æ –≤ –∫—ç—à–µ\n",
    "        [5] –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å–ª–æ–≤–∞\n",
    "        [6] –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
    "\n",
    "    –Ω–∞ –≤—ã—Ö–æ–¥–µ –ª–∏—Å—Ç –æ—Ç–ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
    "    '''\n",
    "\n",
    "    # [0]\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # [1]\n",
    "    tokens = list(tokenize(text))\n",
    "    words = [_.text for _ in tokens]\n",
    "\n",
    "    words_lem = []\n",
    "    for w in words:\n",
    "        if w[0] == '-': # [2]\n",
    "            w = w[1:]\n",
    "        if len(w)>1: # [3]\n",
    "            if w in cache: # [4]\n",
    "                words_lem.append(cache[w])\n",
    "            else: # [5]\n",
    "                temp_cach = cache[w] = morph.parse(w)[0].normal_form \n",
    "                # 0 —Å–∫–æ—Ä–µ —Å–∞–º—ã–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π - –æ—Å—Ç–∞–ª—å–Ω—ã–µ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ —Ñ–æ—Ä–º—ã –ø–µ—Ä–≤–∞—è –≤—Ç–æ—Ä–∞—è –∏ —Ç–¥  —Ä–µ–∂–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è\n",
    "                words_lem.append(temp_cach)\n",
    "    \n",
    "    words_lem_without_stopwords=[i for i in words_lem if not i in stopwords] # [6]\n",
    "    \n",
    "    return words_lem_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57b92384",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_russian = df.loc[df.language == 'ENGLISH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab0b2b1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-a831bfacbccc>:15: FutureWarning: Possible nested set at position 39\n",
      "  text = re.sub(\"[0-9]|[-‚Äî.,:;_%¬©¬´¬ª?*!@#‚Ññ$^‚Ä¢¬∑&()]|[+=]|[[]|[]]|[/]|ÔøΩ\", '', text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.95 s, sys: 64.7 ms, total: 3.01 s\n",
      "Wall time: 3.01 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#–ó–∞–ø—É—Å–∫–∞–µ–º –æ—á–∏—Å—Ç–∫—É —Ç–µ–∫—Å—Ç–∞. –ë—É–¥–µ—Ç –¥–æ–ª–≥–æ...\n",
    "df_russian['article_content'] = df_russian['article_content'].apply(lambda x: clean_text(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf31c99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #–ó–∞–ø—É—Å–∫–∞–µ–º –æ—á–∏—Å—Ç–∫—É —Ç–µ–∫—Å—Ç–∞. –ë—É–¥–µ—Ç –¥–æ–ª–≥–æ...\n",
    "# df['article_content'] = df['article_content'].apply(lambda x: clean_text(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9defeac6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #–ó–∞–ø—É—Å–∫–∞–µ–º –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞. –ë—É–¥–µ—Ç –æ—á–µ–Ω—å –¥–æ–ª–≥–æ...\n",
    "# df['article_content'] = df['article_content'].apply(lambda x: lemmatization(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf9148ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "no such name",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/shapes.py\u001b[0m in \u001b[0;36mis_latin_char\u001b[0;34m(uchr)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_latin_letters_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muchr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'òÇá'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4136\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4137\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4138\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-a831bfacbccc>\u001b[0m in \u001b[0;36mlemmatization\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mwords_lem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# [5]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mtemp_cach\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmorph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0;31m# 0 —Å–∫–æ—Ä–µ —Å–∞–º—ã–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π - –æ—Å—Ç–∞–ª—å–Ω—ã–µ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ —Ñ–æ—Ä–º—ã –ø–µ—Ä–≤–∞—è –≤—Ç–æ—Ä–∞—è –∏ —Ç–¥  —Ä–µ–∂–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mwords_lem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_cach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/analyzer.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_terminal\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_units\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_terminal\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/units/by_shape.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, word, word_lower, seen_parses)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseen_parses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_lower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/units/by_shape.py\u001b[0m in \u001b[0;36mcheck_shape\u001b[0;34m(self, word, word_lower)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_lower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mis_latin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/shapes.py\u001b[0m in \u001b[0;36mis_latin\u001b[0;34m(token)\u001b[0m\n\u001b[1;32m     38\u001b[0m     return (\n\u001b[1;32m     39\u001b[0m         \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_latin_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/shapes.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     38\u001b[0m     return (\n\u001b[1;32m     39\u001b[0m         \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_latin_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/shapes.py\u001b[0m in \u001b[0;36mis_latin_char\u001b[0;34m(uchr)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muchr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0muchr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muchr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mis_latin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'LATIN'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muchr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_latin_letters_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muchr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_latin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: no such name"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#–ó–∞–ø—É—Å–∫–∞–µ–º –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞. –ë—É–¥–µ—Ç –æ—á–µ–Ω—å –¥–æ–ª–≥–æ...\n",
    "df_russian['article_content'] = df_russian['article_content'].apply(lambda x: lemmatization(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22c51d66",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-2edf55d01784>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create a corpus from a list of texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcommon_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mcommon_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcommon_dictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# common_corpus —ç—Ç–æ –∑–µ–ª–µ–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ - –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –æ–Ω –±—É–¥–µ—Ç —Å—Ç—Ä–æ–∏—Ç—å —á—Ç–æ –∑–∞ —Å–ª–æ–≤–∞ –≤ –∫–∞–∂–¥–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             self.add_lifecycle_event(\n\u001b[1;32m     80\u001b[0m                 \u001b[0;34m\"created\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;31m# update Dictionary with the document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ignore the result, here we only care about updating token ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"built %s from %i documents (total %i corpus positions)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \"\"\"\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"doc2bow expects an array of unicode tokens on input, not a single string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;31m# Construct (word, frequency) mapping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "#—Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –Ω–∞—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤, —Ä–∞–∑–±–∏–≤ –µ—â–µ –∏ –Ω–∞ –ø—Ä–æ–±–µ–ª—ã\n",
    "texts = [t for t in df_russian['article_content'].values]\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in texts] \n",
    "# common_corpus —ç—Ç–æ –∑–µ–ª–µ–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ - –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –æ–Ω –±—É–¥–µ—Ç —Å—Ç—Ä–æ–∏—Ç—å —á—Ç–æ –∑–∞ —Å–ª–æ–≤–∞ –≤ –∫–∞–∂–¥–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ\n",
    "# —Ç–µ–∫—Å—Ç –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –±—Ä–æ—Å–∞–µ–º –≤ doc2bow –≤ –Ω–∞—à common_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33af593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660dbb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_number = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f0d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.models import LdaModel\n",
    "# Train the model on the corpus.\n",
    "\n",
    "# —ç—Ç–æ –Ω–µ —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ - —ç—Ç–æ —Å—Ä–∞–∑—É –∏ –æ–±—É—á–µ–Ω–∏–µ!!!\n",
    "lda = LdaModel(common_corpus, num_topics=topics_number, id2word=common_dictionary)#, passes=10) \n",
    "# passes - —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –±—É–¥–µ–º —Å–¥–≤–∏–≥–∞—Ç—å —Ü–µ–Ω—Ç—Ä–æ–∏–¥—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaed4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "# Save model to disk.\n",
    "temp_file = datapath(\"model.lda\")\n",
    "lda.save(temp_file)\n",
    "\n",
    "# Load a potentially pretrained model from disk.\n",
    "lda = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e7260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new corpus, made of previously unseen documents.\n",
    "other_texts = [t for t in df_russian['article_content'].iloc[:3]]\n",
    "other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]\n",
    "\n",
    "unseen_doc = other_corpus[2]\n",
    "print(other_texts[2])\n",
    "lda[unseen_doc] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2170e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=lda.show_topics(num_topics=topics_number, num_words=7,formatted=False)\n",
    "topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "\n",
    "#Below Code Prints Only Words \n",
    "for topic,words in topics_words:\n",
    "    print(\"topic_{}: \".format(topic)+\" \".join(words))\n",
    "# –ø–æ–ª—É—á–∞–µ—Ç—Å—è —ç—Ç–æ –≤—ã–≤–µ–ª–∏ —Ç–æ–ø 7 —Å–ª–æ–≤ –≤ –∫–∞–∂–¥–æ–π —Ç–µ–º–∞—Ç–∏–∫–µ –ø–æ –≤–µ–ª–∏—á–∏–Ω–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏ –∫ —ç—Ç–æ–π —Ç–µ–º–µ\n",
    "# —Ç–∞–∫–∏–µ –≤–µ—â–∏ –∫–∞–∫ nn –Ω–∞–¥–æ –∑–∞–Ω–æ—Å–∏—Ç—å –≤ —Å—Ç–æ–ø —Å–ª–æ–≤–∞ - —Å–Ω–æ–≤–∞ –ø—Ä–æ–≥–æ–Ω–∏—Ç–µ - \n",
    "# –∏ –æ–ø—è—Ç—å –º–æ–≥—É—Ç –º—É—Å–æ—Ä–Ω—ã–µ —Å–ª–æ–≤–∞ –±—ã—Ç—å - —ç—Ç–æ –Ω–æ–æ—Ä–º–∞–ª—å–Ω–æ - —ç—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –¥–ª—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "# –¥–ª—è —Ç–µ–º–∞—Ç–∏—á –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∫—Ä–∞–π–Ω–µ –≤–∞–∂–Ω–æ —Å–∏–¥–µ—Ç—å –∏ –∏—Å–∫–ª—é—á–∞—Ç—å –º—É—Å–æ—Ä–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã - –º–ª–Ω –∏ —Ç–¥\n",
    "# –ø—Ä–æ—Ü–µ—Å—Å –ø–æ—Å—Ç—Ä–æ–π–∫–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–æ–ª–≥–∏–π –∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π\n",
    "\n",
    "# –º—ã –∫–∏–¥–∞–µ–º —Ü–µ–Ω—Ç—Ä–æ–∏–¥—ã –∏ –µ—Å–ª–∏ –æ–Ω–∏ –±—É–¥—É—Ç —Å–∫–∞—Ç—ã–≤–∞—Ç—å—Å—è –∫ —Ä–∞–∑–Ω—ã–º –º–∏–Ω–∏–º—É–º–∞–º —ç—Ç–æ —Ç–æ–∂–µ –Ω–æ—Ä–º–∞–ª—å–Ω–æ - —Ä–∞–∑–Ω—ã–µ —Å–ª–æ–≤–∞ –º–æ–≥—É—Ç –±—ã—Ç—å\n",
    "# –∏–∑–∑–∞ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ (–∫–∏–¥–∞–µ–º —Ü–µ–Ω—Ç—Ä–æ–∏–¥—ã) —Ç–æ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ—Å—Ç—å –Ω–µ –±—É–¥–µ—Ç –∏ —Å–∫–∞—Ç—ã–≤–∞—Ç—å—Å—è –±—É–¥–µ—Ç –≤ —Ä–∞–∑–Ω–µ –º–µ—Å—Ç–∞\n",
    "\n",
    "# –º–æ–∂–Ω–æ –∑–∞–ø–∏—Å–∞—Ç—å –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ü–µ–Ω—Ç—Ä–æ–∏–¥—ã –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ª—É—á–∏–ª–∏ –∏ \n",
    "# –µ—Å—Ç—å –º–µ—Ç–æ–¥—ã –∫–æ—Ç–æ—Ä—ã–µ –æ–±—É—á–∞—é—Ç—Å—è —Å –Ω—É–ª—è –Ω–æ –∏–º–µ—é—Ç –Ω–∞—á–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏–º–µ–Ω–Ω–æ –≤ —ç—Ç–∏—Ö —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539239a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a287aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
